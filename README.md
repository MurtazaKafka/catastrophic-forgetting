# EWC Approach to Mitigate Catastrophic Forgetting


## Core Components

### 1. Baseline Methods
- Elastic Weight Consolidation (EWC)
- Activation Sharpening
- Progressive Neural Networks

### 2. Experiments
- Sequential MNIST
- Permuted MNIST
- Continual Learning Benchmarks

## Results

### Performance Metrics
- Task accuracy retention
- Forgetting ratio
- Synaptic stability

### Visualization
- Learning curves
- Weight distribution analysis
- Task interference patterns

## Contribute by
1. Forking repository
2. Creating feature branch
3. Submitting pull requests

## Contact
Murtaza Nikzad
Email: [munikzad@davidson.edu]



## Course Description
This independent research course explores advanced techniques in machine learning, focusing on mitigating catastrophic forgetting in artificial neural networks. Students will investigate Hebbian learning and Spike-Timing-Dependent Plasticity (STDP) inspired approaches to address this critical challenge in continual learning.

## Course Objectives
By the end of this course, students will be able to:
1. Understand the history and significance of catastrophic forgetting in neural networks
2. Analyze the mathematical foundations of catastrophic forgetting
3. Evaluate various approaches to mitigate catastrophic forgetting
4. Implement and test Hebbian/STDP-inspired algorithms
5. Conduct independent research and present findings

## Course Structure
This 15-week course is divided into three main phases:
1. Foundations (Weeks 1-5)
2. Advanced Concepts and Implementation (Weeks 6-10)
3. Independent Research and Presentation (Weeks 11-15)

## Weekly Schedule

### Week 1: Introduction to Catastrophic Forgetting
- Topics:
  - Historical context of catastrophic forgetting
  - Overview of continual learning challenges
- Readings:
  - McCloskey, M., & Cohen, N. J. (1989). Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem. Psychology of Learning and Motivation, 24, 109-165.
  - Goodfellow, I. J., Mirza, M., Xiao, D., Courville, A., & Bengio, Y. (2013). An empirical investigation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211.
- Assignment:
  - Write a 2-page summary of the McCloskey & Cohen paper, highlighting key findings and their relevance to modern machine learning.

### Week 2: Mathematical Foundations and Taxonomy of Mitigation Approaches
- Topics:
  - Mathematical formulation of catastrophic forgetting
  - Overview of mitigation strategies
- Readings:
  - French, R. M. (1999). Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4), 128-135.
  - Parisi, G. I., Kemker, R., Part, J. L., Kanan, C., & Wermter, S. (2019). Continual lifelong learning with neural networks: A review. Neural Networks, 113, 54-71.
- Assignment:
  - Create a taxonomy diagram of different approaches to mitigate catastrophic forgetting, including brief descriptions of each approach.

### Week 3: Hebbian Learning and Synaptic Plasticity
- Topics:
  - Principles of Hebbian learning
  - Biological basis of synaptic plasticity
- Readings:
  - Hebb, D. O. (1949). The organization of behavior: A neuropsychological theory. New York: Wiley. (Selected chapters)
  - Abbott, L. F., & Nelson, S. B. (2000). Synaptic plasticity: taming the beast. Nature neuroscience, 3(11), 1178-1183.
- Assignment:
  - Implement a simple Hebbian learning algorithm in Python and demonstrate its basic properties.

### Week 4: Spike-Timing-Dependent Plasticity (STDP)
- Topics:
  - STDP mechanisms and models
  - Relevance to artificial neural networks
- Readings:
  - Bi, G. Q., & Poo, M. M. (1998). Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type. Journal of neuroscience, 18(24), 10464-10472.
  - Song, S., Miller, K. D., & Abbott, L. F. (2000). Competitive Hebbian learning through spike-timing-dependent synaptic plasticity. Nature neuroscience, 3(9), 919-926.
- Assignment:
  - Write a report comparing and contrasting Hebbian learning and STDP, focusing on their potential applications in artificial neural networks.

### Week 5: Review and Research Proposal
- Topics:
  - Review of Weeks 1-4
  - Introduction to research methodologies
- Readings:
  - Selected papers based on individual research interests
- Assignment:
  - Develop a research proposal (3-5 pages) outlining a potential Hebbian/STDP-inspired approach to mitigate catastrophic forgetting.

### Week 6-7: Elastic Weight Consolidation and Synaptic Intelligence
- Topics:
  - EWC algorithm and its variants
  - Synaptic intelligence approach
- Readings:
  - Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hadsell, R. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the national academy of sciences, 114(13), 3521-3526.
  - Zenke, F., Poole, B., & Ganguli, S. (2017). Continual learning through synaptic intelligence. In International Conference on Machine Learning (pp. 3987-3995). PMLR.
- Assignment:
  - Implement and compare EWC and synaptic intelligence on a simple continual learning task.

### Week 8-9: Memory-based Approaches and Generative Replay
- Topics:
  - Episodic memory in continual learning
  - Generative replay techniques
- Readings:
  - Lopez-Paz, D., & Ranzato, M. A. (2017). Gradient episodic memory for continual learning. Advances in neural information processing systems, 30.
  - Shin, H., Lee, J. K., Kim, J., & Kim, J. (2017). Continual learning with deep generative replay. Advances in neural information processing systems, 30.
- Assignment:
  - Design and implement a simple memory-based or generative replay approach for continual learning.

### Week 10: Hebbian/STDP-Inspired Approaches in Continual Learning
- Topics:
  - Recent developments in Hebbian/STDP-inspired continual learning
  - Challenges and opportunities
- Readings:
  - Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., & Tuytelaars, T. (2018). Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV) (pp. 139-154).
  - Mandge, D., & Manchanda, M. (2022). Continual learning with adaptive synapses and spiking neural networks. Frontiers in Neuroscience, 16, 855613.
- Assignment:
  - Critically analyze a recent Hebbian/STDP-inspired approach to continual learning and propose potential improvements.

### Week 11-13: Independent Research
- Topics:
  - Implementation of proposed approach
  - Experimental design and evaluation
- Readings:
  - Customized based on individual research direction
- Assignment:
  - Weekly progress reports and code submissions

### Week 14: Results Analysis and Paper Writing
- Topics:
  - Statistical analysis of results
  - Scientific writing and visualization
- Readings:
  - Guidelines on scientific writing and result presentation
- Assignment:
  - Draft of research paper (6-8 pages, conference paper format)

### Week 15: Final Presentation and Submission
- Topics:
  - Effective research presentation
  - Peer review process
- Assignment:
  - Final research paper submission
  - 20-minute presentation of research findings

## Grading
- Weekly Assignments and Participation: 30%
- Research Proposal: 15%
- Implementation and Experiments: 25%
- Final Paper: 20%
- Final Presentation: 10%

## Resources
- TensorFlow or PyTorch for neural network implementation
- Jupyter Notebooks for interactive development and visualization
- GitHub for version control and code sharing
- Overleaf for collaborative LaTeX writing

## Office Hours
By appointment, to be scheduled via email.

Note: This syllabus is subject to change based on the progress and interests of the student. Any modifications will be discussed and agreed upon by both the instructor and the student.
